{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOz5ZPbnFiFl5KLPdR7ArfJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuxuanLiu0622/ECE50024-Project-Team15/blob/main/checkpoint3_reweight_upload.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checkpoint 3: Reimplementation of Learning to Reweight Examples for Robust Deep Learning on Toy Problems (FashionMNIST) Part 2: CNN with reweighting\n",
        "\n",
        "**Team15: Hyun Soo Park, Andres Martinez, Heesoo Kim, Mingyu Kim, Yuxuan Liu**"
      ],
      "metadata": {
        "id": "m-b4dxtChUhB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFwjJwthJkf2",
        "outputId": "08e044e5-d788-4211-8e55-7a3fa1b6ec06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm\n",
        "import time\n",
        "from typing import List, Dict\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "import IPython"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install higher"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sCk5A4dM091",
        "outputId": "afd1e781-fec4-4764-cd9a-8d949c3f4a37"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting higher\n",
            "  Downloading higher-0.2.1-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from higher) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->higher) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->higher) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->higher) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->higher) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->higher) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->higher) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->higher) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->higher) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->higher) (1.3.0)\n",
            "Installing collected packages: higher\n",
            "Successfully installed higher-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import higher"
      ],
      "metadata": {
        "id": "QPbLU8hFM3_N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "Xj5RWSeJJmFq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int = 0) -> None:\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "set_seed(0)"
      ],
      "metadata": {
        "id": "6jYW2T6CJnHo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    self.conv = nn.Conv2d(1, 8, kernel_size=3)\n",
        "    self.conv2 = nn.Conv2d(8, 16, kernel_size=3)\n",
        "    self.conv3 = nn.Conv2d(16,32,kernel_size=3)\n",
        "    self.conv4 = nn.Conv2d(32,32,kernel_size=3)\n",
        "    self.fc = nn.Linear(512, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.relu(F.max_pool2d(x,2))\n",
        "    x = self.conv3(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv4(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.relu(F.max_pool2d(x,2))\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "sWPvmQHlJrHz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                      torchvision.transforms.Normalize((0.1307,),(0.3081,))])"
      ],
      "metadata": {
        "id": "ulN-BM0AJtPp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_fmnist = torchvision.datasets.FashionMNIST(root=\"data\", train=True, download=True,transform=transform)\n",
        "test_fmnist = torchvision.datasets.FashionMNIST(root=\"data\", train=False, download=True,transform=transform)\n",
        "val_fmnist = torchvision.datasets.FashionMNIST(root=\"data\", train=False, download=True,transform=transform)"
      ],
      "metadata": {
        "id": "QDIDuA4kJucJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d73cd64-cf50-4898-a0be-7e695dda1cee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 19456498.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 331158.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 6064310.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 4585321.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(class1: int, class2: int, imbalance_ratio: float, n_samples: int, train_dataset: torchvision.datasets) -> torch.utils.data.Dataset:\n",
        "    new_data = train_dataset\n",
        "    n_class1 = int(imbalance_ratio*n_samples)\n",
        "    n_class2 = n_samples - n_class1\n",
        "    class1_indices = (train_dataset.targets == class1).nonzero().squeeze()\n",
        "    class2_indices = (train_dataset.targets == class2).nonzero().squeeze()\n",
        "\n",
        "    # Randomly sample indices for each class based on the desired number of samples\n",
        "    selected_class1_indices = class1_indices[torch.randperm(class1_indices.size(0))[:n_class1]]\n",
        "    selected_class2_indices = class2_indices[torch.randperm(class2_indices.size(0))[:n_class2]]\n",
        "\n",
        "    new_data.data = torch.cat((train_dataset.data[selected_class1_indices], train_dataset.data[selected_class2_indices]))\n",
        "\n",
        "    # Update the targets based on the new number of samples\n",
        "    new_data.targets = torch.cat((torch.zeros(selected_class1_indices.size()), torch.ones(selected_class2_indices.size())))\n",
        "    return new_data"
      ],
      "metadata": {
        "id": "FQqABgdDJvS4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training set is given an imbalanced proportion of 99.5%. The validation and testing set is equally distributed."
      ],
      "metadata": {
        "id": "Ap8mK0fJiKPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = split_dataset(7,9,0.9,8000,train_fmnist)\n",
        "test_set = split_dataset(7,9,0.5,1000,test_fmnist)\n",
        "val_set = split_dataset(7,9,0.5,50,val_fmnist)"
      ],
      "metadata": {
        "id": "WeHkgPTyJw1c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\n",
        "    'lr' : 1e-3,\n",
        "    'momentum' : 0.9,\n",
        "    'batch_size' : 128,\n",
        "    'epoch' : 5000,\n",
        "}"
      ],
      "metadata": {
        "id": "cY4ynLsdJxqi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset=train_set, batch_size = hyperparameters['batch_size'], shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_set, batch_size=hyperparameters['batch_size'], shuffle=False)\n",
        "val_loader = DataLoader(dataset=val_set, batch_size=hyperparameters['batch_size'], shuffle=False)"
      ],
      "metadata": {
        "id": "NA6PvEfOJyd_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN().to(device)\n",
        "opt = optim.SGD(model.parameters(), lr=hyperparameters['lr'])\n",
        "loss_fn = nn.BCEWithLogitsLoss().to(device)"
      ],
      "metadata": {
        "id": "Ut26mBdrTnhi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply reweighting method to CNN"
      ],
      "metadata": {
        "id": "bxzoM3AMi6NK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(1, hyperparameters['epoch']+1)):\n",
        "  model.train()\n",
        "  train_loss, train_acc = 0, 0\n",
        "  images, labels = next(iter(train_loader))\n",
        "\n",
        "  images_tr = images.to(device)\n",
        "  labels_tr = labels.to(device)\n",
        "\n",
        "  opt.zero_grad()\n",
        "\n",
        "  with higher.innerloop_ctx(model, opt) as (meta_model, meta_opt):\n",
        "    meta_train_outputs = meta_model(images_tr).squeeze()\n",
        "    loss_fn.reduction = 'none'\n",
        "    meta_train_loss = loss_fn(meta_train_outputs, labels.float())\n",
        "    eps = torch.zeros(meta_train_loss.size(), requires_grad=True).to(device)\n",
        "    # construct the computational graph\n",
        "    meta_train_loss = torch.sum(eps * meta_train_loss)\n",
        "    meta_opt.step(meta_train_loss)\n",
        "\n",
        "    images_meta, labels_meta = next(iter(val_loader))\n",
        "    y_g_hat = meta_model(images_meta).squeeze()\n",
        "\n",
        "    loss_fn.reduction = 'mean'\n",
        "    meta_val_loss = loss_fn(y_g_hat, labels_meta.float())\n",
        "    # take the gradient wrt epsilon\n",
        "    eps_grads = torch.autograd.grad(meta_val_loss, eps)[0].detach()\n",
        "  # limit the weight >=0 and normalize it\n",
        "  w_tilde = torch.clamp(-eps_grads, min=0)\n",
        "  l1_norm = torch.sum(w_tilde)\n",
        "  if l1_norm != 0:\n",
        "      w = w_tilde / l1_norm\n",
        "  else:\n",
        "      w = w_tilde\n",
        "\n",
        "  y_f_hat = model(images).squeeze()\n",
        "  loss_f_hat = torch.sum(w * loss_fn(y_f_hat, labels.float()))\n",
        "  loss_f_hat.backward()\n",
        "  opt.step()\n",
        "\n",
        "  train_loss += loss_f_hat.item()\n",
        "  pred_labels = (F.sigmoid(y_f_hat) > 0.5).int()\n",
        "  train_acc += torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "\n",
        "  if i % 10 == 0 and i != 0:\n",
        "      model.eval()\n",
        "      test_acc = []\n",
        "      for i, (images_test, labels_test) in enumerate(test_loader):\n",
        "          images_test = images_test.to(device)\n",
        "          labels_test = labels_test.to(device)\n",
        "\n",
        "          y_hat = model(images_test).to(device)\n",
        "          prediction = (F.sigmoid(y_hat) > 0.5).int()\n",
        "          test_acc.append((torch.flatten(prediction).int() == labels_test.int()).int())\n",
        "\n",
        "      acc = torch.mean(torch.cat(test_acc,dim=0).float())\n",
        "      print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "Ep_RLkm8UH89",
        "outputId": "126284dc-8455-489e-feac-1c04c8b5c489"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 10/5000 [00:03<33:03,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 13/5000 [00:04<26:51,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-b33face3c66f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;31m# print(y_f_hat)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mloss_f_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_f_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0mloss_f_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}